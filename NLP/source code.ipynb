{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1133e90c-f118-40ad-bb97-c0760855e3e4"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=5>\n",
        "    <b>بسم الله الرحمن الرحیم</b>\n",
        "</font>\n",
        "</div>"
      ],
      "id": "1133e90c-f118-40ad-bb97-c0760855e3e4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4eabeea-4937-4e29-a3e8-f175352d4f14"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=5>\n",
        "    <b>چالش درس هوش مصنوعی - پردازش زبان طبیعی</b>\n",
        "    <br>\n",
        "    <b>فاز اول</b>\n",
        "</font>\n",
        "</div>"
      ],
      "id": "b4eabeea-4937-4e29-a3e8-f175352d4f14"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "151f7b7a-ce4a-455a-b202-8185c744c764"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "نوت بوکی که پیش روی شماست، مراحل انجام فاز اول چالش درس هوش مصنوعی در قسمت پردازش زبان طبیعی را تعیین کرده است. در این فاز، شما با استفاده مجموعه‌ای از محتوای متن صفحات وب به همراه برچسب موضوع هر یک، مدلی را توسعه می‌دهید که توانایی دسته‌بندی متن ورودی را به یکی از 16 برچسب مجموعه‌ داده داشته باشد.\n",
        "<br>\n",
        "<br>\n",
        "<b>چند نکته مهم</b>\n",
        "<ul >\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "<li>\n",
        "می‌توانید از کتابخانه‌های با پیاده‌سازی آماده مدل‌های پایه (مانند transformers) استفاده کنید.\n",
        "</li>\n",
        "<li>\n",
        "توجه کنید که تمام کتابخانه های مورد نیاز را نصب کنید و از نصب شدن آن ها اطمینان حاصل کرده و نام آن ها در فایل requirements ذکر کنید.\n",
        "</li>\n",
        "<li>\n",
        "در صورت نیاز به نصب یک کتابخانه یا دسترسی به یک فایل، مراحل نصب و دانلود آن (از یک محل عمومی یا فایل ارسال شده به همراه نوت‌بوک) می‌بایست در نوت‌بوک وجود داشته باشد.\n",
        "</li>\n",
        "<li>\n",
        "تمامی فایل‌های مرتبط به پروژه که دارای حجم کمی هستند، باید به همراه نوت‌بوک در قالب یک فایل زیپ ارسال شوند. فایل‌هایی که حجم زیادی دارند (مانند فایل ذخیره شده یک مدل که بیشتر از ۲۰۰ مگابایت باشد) را در یک محل عمومی مانند گوگل درایو آپلود کرده و لینک دسترسی به آن را در نوت بوک قرار دهید.\n",
        "</li>\n",
        "</font>\n",
        "</ul>\n",
        "</font>\n",
        "</div>"
      ],
      "id": "151f7b7a-ce4a-455a-b202-8185c744c764"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15621f21-c99f-4697-a374-280e29a8b1e4"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "<b>بارگذاری و پیش‌پردازش داده</b>\n",
        "<br>\n",
        "در ابتدا می‌بایست داده‌ها را از فایل با نام dataset_phase1 بارگذاری کنید. سپس برای اینکه بتوانید مدل مناسبی را آموزش دهید،  داده‌ها را پیش پردازش کنید. این قسمت می‌تواند شامل موارد متنوعی باشد؛ از جمله حذف عبارت‌های اضافی موجود در انتهای متن‌ها، حذف یا جداسازی علائم نگارشی، توکن‌بندی کلمات، جایگزینی مواردی مثل اعداد، تاریخ‌ها و آدرس‌‌های الکترونیکی با توکن‌های خاص، حذف فاصله‌های اضافه و ... . برای این منظور، می‌توانید از کتاب‌خانه‌هایی مانند hazm هم استفاده کنید.\n",
        "سپس برای اینکه بتوانید ارزیابی مناسبی از مدل خود داشته باشید، داده‌ها را به دو قسمت آموزش  (train) و ارزیابی  (validation) تقسیم کنید.\n",
        "</font>\n",
        "</div>"
      ],
      "id": "15621f21-c99f-4697-a374-280e29a8b1e4"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install tourch\n",
        "!pip install transformers\n",
        "!pip install hazm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ruhDB15eoLN4",
        "outputId": "51b943e1-b168-420d-ff00-aa9c767e9c22"
      },
      "id": "ruhDB15eoLN4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m225.3/227.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.20.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tourch (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tourch\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.9.0-py3-none-any.whl (477 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m477.8/477.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Collecting numpy<2.0.0,>=1.24.3 (from hazm)\n",
            "  Downloading numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.65.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.1.0)\n",
            "Installing collected packages: python-crfsuite, numpy, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hazm-0.9.0 numpy-1.25.0 python-crfsuite-0.9.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQuVfWD77h_R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "from hazm import Normalizer, word_tokenize, stopwords_list, Stemmer"
      ],
      "id": "KQuVfWD77h_R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "Ll5HUMbQ61dM",
        "outputId": "d9f552b0-560f-4136-d598-4b43ff7b56cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-8f280116a48e>:20: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  dataframe = pd.read_csv('phase_2_train.csv', encoding='utf-8', error_bad_lines=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  class_id class_name\n",
              "0     تداوم فاز انتظار در بازار خودرودنا پلاس امروز ...        21      خودرو\n",
              "1     موتورسیکلت آل با پیشرانه وی ۶ | فیلم نیوزین مو...        21      خودرو\n",
              "2     کدام خودرو در مهر ۱۴۰۱ امکان واردات داشت#دار م...        21      خودرو\n",
              "3     خودرو از ۰ تا ۱۲۷۶۲ میلیون تومان نمایشگاه آنلا...        21      خودرو\n",
              "4     آبروریزی جنجال سایپا در نمایشگاه خودرو گزارش ک...        21      خودرو\n",
              "...                                                 ...       ...        ...\n",
              "3702  رسید درخواست ارزش تحصیل پیام نور رسید درخواست ...         1    تحصیلات\n",
              "3703  پاورپوینت درس ۷ جامعه شناخت#شناس ۳ پایه دوازده...         1    تحصیلات\n",
              "3704  گزارش کارآموزی آماده رشته حسابدار لاین ۲۵ | با...         1    تحصیلات\n",
              "3705  متوسطه پسرانه وکیل شماره ۷ نیشابور دانش‌نامه م...         1    تحصیلات\n",
              "3706  فرصت مجدد انتخاب رشته در کنکور ۱۴۰۱ فراهم شد#ش...         1    تحصیلات\n",
              "\n",
              "[3707 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1451eb55-df22-40f9-af29-265a71057cdd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class_id</th>\n",
              "      <th>class_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>تداوم فاز انتظار در بازار خودرودنا پلاس امروز ...</td>\n",
              "      <td>21</td>\n",
              "      <td>خودرو</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>موتورسیکلت آل با پیشرانه وی ۶ | فیلم نیوزین مو...</td>\n",
              "      <td>21</td>\n",
              "      <td>خودرو</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>کدام خودرو در مهر ۱۴۰۱ امکان واردات داشت#دار م...</td>\n",
              "      <td>21</td>\n",
              "      <td>خودرو</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>خودرو از ۰ تا ۱۲۷۶۲ میلیون تومان نمایشگاه آنلا...</td>\n",
              "      <td>21</td>\n",
              "      <td>خودرو</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>آبروریزی جنجال سایپا در نمایشگاه خودرو گزارش ک...</td>\n",
              "      <td>21</td>\n",
              "      <td>خودرو</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3702</th>\n",
              "      <td>رسید درخواست ارزش تحصیل پیام نور رسید درخواست ...</td>\n",
              "      <td>1</td>\n",
              "      <td>تحصیلات</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3703</th>\n",
              "      <td>پاورپوینت درس ۷ جامعه شناخت#شناس ۳ پایه دوازده...</td>\n",
              "      <td>1</td>\n",
              "      <td>تحصیلات</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3704</th>\n",
              "      <td>گزارش کارآموزی آماده رشته حسابدار لاین ۲۵ | با...</td>\n",
              "      <td>1</td>\n",
              "      <td>تحصیلات</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705</th>\n",
              "      <td>متوسطه پسرانه وکیل شماره ۷ نیشابور دانش‌نامه م...</td>\n",
              "      <td>1</td>\n",
              "      <td>تحصیلات</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3706</th>\n",
              "      <td>فرصت مجدد انتخاب رشته در کنکور ۱۴۰۱ فراهم شد#ش...</td>\n",
              "      <td>1</td>\n",
              "      <td>تحصیلات</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3707 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1451eb55-df22-40f9-af29-265a71057cdd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1451eb55-df22-40f9-af29-265a71057cdd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1451eb55-df22-40f9-af29-265a71057cdd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import Normalizer, Lemmatizer, word_tokenize\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "\n",
        "def normalize(row):\n",
        "    line = re.sub(\n",
        "        r'[.{}[\\]؛:«»؟!٬٫٪×،*)(ـ+<>\\'\",`=+\\-?!@#$%^&*()_\\/\\\\\\\\]', '', str(row['text']).strip())\n",
        "    line = re.sub(r'\\s+', ' ', line.strip())\n",
        "    line = normalizer.normalize(line)\n",
        "    words = word_tokenize(line)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    row['text'] = ' '.join(words)\n",
        "    return row\n",
        "\n",
        "dataframe = pd.read_csv('dataset.csv', encoding='utf-8', error_bad_lines=False)\n",
        "dataframe = dataframe.apply(normalize, axis=1)\n",
        "dataframe"
      ],
      "id": "Ll5HUMbQ61dM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjWHIQR4G1id"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.labels)"
      ],
      "id": "jjWHIQR4G1id"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbDv-ICNw41E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82524264-59a1-4455-85f5-91b7a9f6e620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "dataframe = dataframe.loc[dataframe['class_id'].apply(lambda x: bool(re.match(r'[0-9]+', str(x))))]\n",
        "# Assuming you have a DataFrame called 'dataframe'\n",
        "# Split the DataFrame into 80% train and 20% test\n",
        "train_df, test_df = train_test_split(dataframe, test_size=0.2, random_state=42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# Tokenize the input text and convert labels to numerical values\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "\n",
        "train_input_texts = train_df['text'].tolist()\n",
        "train_labels = [int(id) for id in train_df['class_id'].tolist()]\n",
        "\n",
        "test_input_texts = test_df['text'].tolist()\n",
        "test_labels = [int(id) for id in test_df['class_id'].tolist()]\n",
        "\n",
        "train_encoded_inputs = tokenizer(train_input_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "test_encoded_inputs = tokenizer(test_input_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_dataset = CustomDataset(train_encoded_inputs, train_labels)\n",
        "test_dataset = CustomDataset(test_encoded_inputs, test_labels)\n",
        "\n",
        "# Define the BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(set(train_labels.tolist())))\n",
        "\n",
        "# Move the model to the device\n",
        "model = model.to(device)\n"
      ],
      "id": "zbDv-ICNw41E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvqBp8vcC3Xw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "d90ceb10-bec6-4d23-cf93-6c01a923c576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-4-29ebbf60974d>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-4-29ebbf60974d>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(self.labels[idx])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='366' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [366/930 09:51 < 15:15, 0.62 it/s, Epoch 1.96/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.961271</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-29ebbf60974d>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-4-29ebbf60974d>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(self.labels[idx])\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./outputs\",\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "id": "XvqBp8vcC3Xw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdbbd429-7ce5-42f4-842c-1639bb576b32"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "<b>آموزش و ارزیابی مدل</b>\n",
        "<br>\n",
        "در این قسمت شما به آماده‌سازی مدل انتخابی و آموزش دسته‌بند می‌پردازید. برای این‌که بتوانید به نتایج بهتری برسید، مقادیر مختلف پارامترها و ابرپارامترهای مدل را با استفاده از داده اعتبارسنجی ارزیابی کنید و بهترین مقادیر را انتخاب کنید. در ارزیابی مدل می‌توانید از معیارهای متفاوتی مانند\n",
        "      Recall ،Precision ،Accuracy ، Weighted F1-Scoreو F1-Score استفاده کنید.\n",
        "پس از انتخاب پارامترهای بهینه، می‌توانید مدل نهایی خود را روی تمامی داده‌ها آموزش دهید.\n",
        "</font>\n",
        "</div>"
      ],
      "id": "fdbbd429-7ce5-42f4-842c-1639bb576b32"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69cf4a1d-8920-4b2d-96c2-2a74b797d45b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "174957bd-282a-436b-eb79-3ce894318714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-29ebbf60974d>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-4-29ebbf60974d>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(self.labels[idx])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8100)\n",
            "0.8099730458221024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-af1df97b7280>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  print((torch.tensor(predicted) == torch.tensor(test_labels)).sum()/len(predicted))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy\n",
        "\n",
        "predicted = trainer.predict(test_dataset).predictions.argmax(axis=1)\n",
        "f1 = f1_score(test_labels, predicted.tolist(), average='micro')\n",
        "print(f1)"
      ],
      "id": "69cf4a1d-8920-4b2d-96c2-2a74b797d45b"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}